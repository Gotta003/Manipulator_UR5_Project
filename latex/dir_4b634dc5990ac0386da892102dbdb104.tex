\doxysection{robotics\+\_\+project\+\_\+ws/src/vision\+\_\+ws/yolov5/utils/loggers/clearml Directory Reference}
\hypertarget{dir_4b634dc5990ac0386da892102dbdb104}{}\label{dir_4b634dc5990ac0386da892102dbdb104}\index{robotics\_project\_ws/src/vision\_ws/yolov5/utils/loggers/clearml Directory Reference@{robotics\_project\_ws/src/vision\_ws/yolov5/utils/loggers/clearml Directory Reference}}
\doxysubsubsection*{Files}
\begin{DoxyCompactItemize}
\item 
file \mbox{\hyperlink{yolov5_2utils_2loggers_2clearml_2____init_____8py}{\+\_\+\+\_\+init\+\_\+\+\_\+.\+py}}
\item 
file \mbox{\hyperlink{clearml__utils_8py}{clearml\+\_\+utils.\+py}}
\item 
file \mbox{\hyperlink{clearml_2hpo_8py}{hpo.\+py}}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\hypertarget{README.md_autotoc_md78}{}\doxysubsection{\texorpdfstring{About Clear\+ML}{About Clear\+ML}}\label{README.md_autotoc_md78}
\href{https://clear.ml/}{\texttt{ Clear\+ML}} is an \href{https://github.com/allegroai/clearml}{\texttt{ open-\/source}} toolbox designed to save you time ‚è±Ô∏è.

üî® Track every YOLOv5 training run in the {\bfseries{experiment manager}}

üîß Version and easily access your custom training data with the integrated Clear\+ML {\bfseries{Data Versioning Tool}}

üî¶ {\bfseries{Remotely train and monitor}} your YOLOv5 training runs using Clear\+ML Agent

üî¨ Get the very best m\+AP using Clear\+ML {\bfseries{Hyperparameter Optimization}}

üî≠ Turn your newly trained {\bfseries{YOLOv5 model into an API}} with just a few commands using Clear\+ML Serving

And so much more. It\textquotesingle{}s up to you how many of these tools you want to use, you can stick to the experiment manager, or chain them all together into an impressive pipeline!

\hypertarget{README.md_autotoc_md79}{}\doxysubsection{\texorpdfstring{ü¶æ Setting Things Up}{ü¶æ Setting Things Up}}\label{README.md_autotoc_md79}
To keep track of your experiments and/or data, Clear\+ML needs to communicate to a server. You have 2 options to get one\+:

Either sign up for free to the \href{https://clear.ml/}{\texttt{ Clear\+ML Hosted Service}} or you can set up your own server, see \href{https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server}{\texttt{ here}}. Even the server is open-\/source, so even if you\textquotesingle{}re dealing with sensitive data, you should be good to go!


\begin{DoxyEnumerate}
\item Install the {\ttfamily clearml} python package\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{pip\ install\ clearml}

\end{DoxyCode}

\item Connect the Clear\+ML SDK to the server by \href{https://app.clear.ml/settings/workspace-configuration}{\texttt{ creating credentials}} (go right top to Settings -\/\texorpdfstring{$>$}{>} Workspace -\/\texorpdfstring{$>$}{>} Create new credentials), then execute the command below and follow the instructions\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{clearml-\/init}

\end{DoxyCode}

\end{DoxyEnumerate}

That\textquotesingle{}s it! You\textquotesingle{}re done üòé\hypertarget{README.md_autotoc_md80}{}\doxysubsection{\texorpdfstring{üöÄ Training YOLOv5 With Clear\+ML}{üöÄ Training YOLOv5 With Clear\+ML}}\label{README.md_autotoc_md80}
To enable Clear\+ML experiment tracking, simply install the Clear\+ML pip package.


\begin{DoxyCode}{0}
\DoxyCodeLine{pip\ install\ clearml>=1.2.0}

\end{DoxyCode}


This will enable integration with the YOLOv5 training script. Every training run from now on, will be captured and stored by the Clear\+ML experiment manager.

If you want to change the {\ttfamily project\+\_\+name} or {\ttfamily task\+\_\+name}, use the {\ttfamily -\/-\/project} and {\ttfamily -\/-\/name} arguments of the {\ttfamily train.\+py} script, by default the project will be called {\ttfamily YOLOv5} and the task {\ttfamily Training}. PLEASE NOTE\+: Clear\+ML uses {\ttfamily /} as a delimiter for subprojects, so be careful when using {\ttfamily /} in your project name!


\begin{DoxyCode}{0}
\DoxyCodeLine{python\ train.py\ -\/-\/img\ 640\ -\/-\/batch\ 16\ -\/-\/epochs\ 3\ -\/-\/data\ coco128.yaml\ -\/-\/weights\ yolov5s.pt\ -\/-\/cache}

\end{DoxyCode}


or with custom project and task name\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{python\ train.py\ -\/-\/project\ my\_project\ -\/-\/name\ my\_training\ -\/-\/img\ 640\ -\/-\/batch\ 16\ -\/-\/epochs\ 3\ -\/-\/data\ coco128.yaml\ -\/-\/weights\ yolov5s.pt\ -\/-\/cache}

\end{DoxyCode}


This will capture\+:


\begin{DoxyItemize}
\item Source code + uncommitted changes
\item Installed packages
\item (Hyper)parameters
\item Model files (use {\ttfamily -\/-\/save-\/period n} to save a checkpoint every n epochs)
\item Console output
\item Scalars (m\+AP\+\_\+0.\+5, m\+AP\+\_\+0.\+5\+:0.\+95, precision, recall, losses, learning rates, ...)
\item General info such as machine details, runtime, creation date etc.
\item All produced plots such as label correlogram and confusion matrix
\item Images with bounding boxes per epoch
\item Mosaic per epoch
\item Validation images per epoch
\item ...
\end{DoxyItemize}

That\textquotesingle{}s a lot right? ü§Ø Now, we can visualize all of this information in the Clear\+ML UI to get an overview of our training progress. Add custom columns to the table view (such as e.\+g. m\+AP\+\_\+0.\+5) so you can easily sort on the best performing model. Or select multiple experiments and directly compare them!

There even more we can do with all of this information, like hyperparameter optimization and remote execution, so keep reading if you want to see how that works!\hypertarget{README.md_autotoc_md81}{}\doxysubsection{\texorpdfstring{üîó Dataset Version Management}{üîó Dataset Version Management}}\label{README.md_autotoc_md81}
Versioning your data separately from your code is generally a good idea and makes it easy to acquire the latest version too. This repository supports supplying a dataset version ID, and it will make sure to get the data if it\textquotesingle{}s not there yet. Next to that, this workflow also saves the used dataset ID as part of the task parameters, so you will always know for sure which data was used in which experiment!

\hypertarget{README.md_autotoc_md82}{}\doxysubsubsection{\texorpdfstring{Prepare Your Dataset}{Prepare Your Dataset}}\label{README.md_autotoc_md82}
The YOLOv5 repository supports a number of different datasets by using yaml files containing their information. By default datasets are downloaded to the {\ttfamily ../datasets} folder in relation to the repository root folder. So if you downloaded the {\ttfamily coco128} dataset using the link in the yaml or with the scripts provided by yolov5, you get this folder structure\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{..}
\DoxyCodeLine{|\_\ yolov5}
\DoxyCodeLine{|\_\ datasets}
\DoxyCodeLine{\ \ \ \ |\_\ coco128}
\DoxyCodeLine{\ \ \ \ \ \ \ \ |\_\ images}
\DoxyCodeLine{\ \ \ \ \ \ \ \ |\_\ labels}
\DoxyCodeLine{\ \ \ \ \ \ \ \ |\_\ LICENSE}
\DoxyCodeLine{\ \ \ \ \ \ \ \ |\_\ README.txt}

\end{DoxyCode}


But this can be any dataset you wish. Feel free to use your own, as long as you keep to this folder structure.

Next, ‚ö†Ô∏è\texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}copy the corresponding yaml file to the root of the dataset folder\texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}‚ö†Ô∏è. This yaml files contains the information Clear\+ML will need to properly use the dataset. You can make this yourself too, of course, just follow the structure of the example yamls.

Basically we need the following keys\+: {\ttfamily path}, {\ttfamily train}, {\ttfamily test}, {\ttfamily val}, {\ttfamily nc}, {\ttfamily names}.


\begin{DoxyCode}{0}
\DoxyCodeLine{..}
\DoxyCodeLine{|\_\ yolov5}
\DoxyCodeLine{|\_\ datasets}
\DoxyCodeLine{\ \ \ \ |\_\ coco128}
\DoxyCodeLine{\ \ \ \ \ \ \ \ |\_\ images}
\DoxyCodeLine{\ \ \ \ \ \ \ \ |\_\ labels}
\DoxyCodeLine{\ \ \ \ \ \ \ \ |\_\ coco128.yaml\ \ \#\ <-\/-\/-\/-\/\ HERE!}
\DoxyCodeLine{\ \ \ \ \ \ \ \ |\_\ LICENSE}
\DoxyCodeLine{\ \ \ \ \ \ \ \ |\_\ README.txt}

\end{DoxyCode}
\hypertarget{README.md_autotoc_md83}{}\doxysubsubsection{\texorpdfstring{Upload Your Dataset}{Upload Your Dataset}}\label{README.md_autotoc_md83}
To get this dataset into Clear\+ML as a versioned dataset, go to the dataset root folder and run the following command\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{cd\ coco128}
\DoxyCodeLine{clearml-\/data\ sync\ -\/-\/project\ YOLOv5\ -\/-\/name\ coco128\ -\/-\/folder\ .}

\end{DoxyCode}


The command {\ttfamily clearml-\/data sync} is actually a shorthand command. You could also run these commands one after the other\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\#\ Optionally\ add\ -\/-\/parent\ <parent\_dataset\_id>\ if\ you\ want\ to\ base}
\DoxyCodeLine{\#\ this\ version\ on\ another\ dataset\ version,\ so\ no\ duplicate\ files\ are\ uploaded!}
\DoxyCodeLine{clearml-\/data\ create\ -\/-\/name\ coco128\ -\/-\/project\ YOLOv5}
\DoxyCodeLine{clearml-\/data\ add\ -\/-\/files\ .}
\DoxyCodeLine{clearml-\/data\ close}

\end{DoxyCode}
\hypertarget{README.md_autotoc_md84}{}\doxysubsubsection{\texorpdfstring{Run Training Using A Clear\+ML Dataset}{Run Training Using A Clear\+ML Dataset}}\label{README.md_autotoc_md84}
Now that you have a Clear\+ML dataset, you can very simply use it to train custom YOLOv5 üöÄ models!


\begin{DoxyCode}{0}
\DoxyCodeLine{python\ train.py\ -\/-\/img\ 640\ -\/-\/batch\ 16\ -\/-\/epochs\ 3\ -\/-\/data\ clearml://<your\_dataset\_id>\ -\/-\/weights\ yolov5s.pt\ -\/-\/cache}

\end{DoxyCode}
\hypertarget{README.md_autotoc_md85}{}\doxysubsection{\texorpdfstring{üëÄ Hyperparameter Optimization}{üëÄ Hyperparameter Optimization}}\label{README.md_autotoc_md85}
Now that we have our experiments and data versioned, it\textquotesingle{}s time to take a look at what we can build on top!

Using the code information, installed packages and environment details, the experiment itself is now {\bfseries{completely reproducible}}. In fact, Clear\+ML allows you to clone an experiment and even change its parameters. We can then just rerun it with these new parameters automatically, this is basically what HPO does!

To {\bfseries{run hyperparameter optimization locally}}, we\textquotesingle{}ve included a pre-\/made script for you. Just make sure a training task has been run at least once, so it is in the Clear\+ML experiment manager, we will essentially clone it and change its hyperparameters.

You\textquotesingle{}ll need to fill in the ID of this {\ttfamily template task} in the script found at {\ttfamily \doxylink{clearml_2hpo_8py}{utils/loggers/clearml/hpo.\+py}} and then just run it \+:) You can change {\ttfamily task.\+execute\+\_\+locally()} to {\ttfamily task.\+execute()} to put it in a Clear\+ML queue and have a remote agent work on it instead.


\begin{DoxyCode}{0}
\DoxyCodeLine{\#\ To\ use\ optuna,\ install\ it\ first,\ otherwise\ you\ can\ change\ the\ optimizer\ to\ just\ be\ RandomSearch}
\DoxyCodeLine{pip\ install\ optuna}
\DoxyCodeLine{python\ utils/loggers/clearml/hpo.py}

\end{DoxyCode}


\hypertarget{README.md_autotoc_md86}{}\doxysubsection{\texorpdfstring{ü§Ø Remote Execution (advanced)}{ü§Ø Remote Execution (advanced)}}\label{README.md_autotoc_md86}
Running HPO locally is really handy, but what if we want to run our experiments on a remote machine instead? Maybe you have access to a very powerful GPU machine on-\/site, or you have some budget to use cloud GPUs. This is where the Clear\+ML Agent comes into play. Check out what the agent can do here\+:


\begin{DoxyItemize}
\item \href{https://www.youtube.com/watch?v=MX3BrXnaULs&feature=youtu.be}{\texttt{ You\+Tube video}}
\item \href{https://clear.ml/docs/latest/docs/clearml_agent}{\texttt{ Documentation}}
\end{DoxyItemize}

In short\+: every experiment tracked by the experiment manager contains enough information to reproduce it on a different machine (installed packages, uncommitted changes etc.). So a Clear\+ML agent does just that\+: it listens to a queue for incoming tasks and when it finds one, it recreates the environment and runs it while still reporting scalars, plots etc. to the experiment manager.

You can turn any machine (a cloud VM, a local GPU machine, your own laptop ... ) into a Clear\+ML agent by simply running\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{clearml-\/agent\ daemon\ -\/-\/queue\ <queues\_to\_listen\_to>\ [-\/-\/docker]}

\end{DoxyCode}
\hypertarget{README.md_autotoc_md87}{}\doxysubsubsection{\texorpdfstring{Cloning, Editing And Enqueuing}{Cloning, Editing And Enqueuing}}\label{README.md_autotoc_md87}
With our agent running, we can give it some work. Remember from the HPO section that we can clone a task and edit the hyperparameters? We can do that from the interface too!

ü™Ñ Clone the experiment by right-\/clicking it

üéØ Edit the hyperparameters to what you wish them to be

‚è≥ Enqueue the task to any of the queues by right-\/clicking it

\hypertarget{README.md_autotoc_md88}{}\doxysubsubsection{\texorpdfstring{Executing A Task Remotely}{Executing A Task Remotely}}\label{README.md_autotoc_md88}
Now you can clone a task like we explained above, or simply mark your current script by adding {\ttfamily task.\+execute\+\_\+remotely()} and on execution it will be put into a queue, for the agent to start working on!

To run the YOLOv5 training script remotely, all you have to do is add this line to the training.\+py script after the clearml logger has been instantiated\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{comment}{\#\ ...}}
\DoxyCodeLine{\textcolor{comment}{\#\ Loggers}}
\DoxyCodeLine{data\_dict\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\textcolor{keywordflow}{if}\ RANK\ \textcolor{keywordflow}{in}\ \{-\/1,\ 0\}:}
\DoxyCodeLine{\ \ \ \ loggers\ =\ Loggers(save\_dir,\ weights,\ opt,\ hyp,\ LOGGER)\ \ \textcolor{comment}{\#\ loggers\ instance}}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordflow}{if}\ loggers.clearml:}
\DoxyCodeLine{\ \ \ \ \ \ \ \ loggers.clearml.task.execute\_remotely(queue=\textcolor{stringliteral}{"{}my\_queue"{}})\ \ \textcolor{comment}{\#\ <-\/-\/-\/-\/-\/-\/\ ADD\ THIS\ LINE}}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Data\_dict\ is\ either\ None\ is\ user\ did\ not\ choose\ for\ ClearML\ dataset\ or\ is\ filled\ in\ by\ ClearML}}
\DoxyCodeLine{\ \ \ \ \ \ \ \ data\_dict\ =\ loggers.clearml.data\_dict}
\DoxyCodeLine{\textcolor{comment}{\#\ ...}}

\end{DoxyCode}


When running the training script after this change, python will run the script up until that line, after which it will package the code and send it to the queue instead!\hypertarget{README.md_autotoc_md89}{}\doxysubsubsection{\texorpdfstring{Autoscaling workers}{Autoscaling workers}}\label{README.md_autotoc_md89}
Clear\+ML comes with autoscalers too! This tool will automatically spin up new remote machines in the cloud of your choice (AWS, GCP, Azure) and turn them into Clear\+ML agents for you whenever there are experiments detected in the queue. Once the tasks are processed, the autoscaler will automatically shut down the remote machines, and you stop paying!

Check out the autoscalers getting started video below.

\href{https://youtu.be/j4XVMAaUt3E}{\texttt{ }} 