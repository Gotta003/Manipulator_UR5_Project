\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[hidelinks]{hyperref}
\usepackage{geometry}
\geometry{margin=1in}

% Title and Author
\title{Robotics Project}
\author{Benassi Alessandro, Calvo Daniele, Cristoforetti Niccol√≤, Gottardelli Matteo}
\date{\today}

% Begin Document
\begin{document}

% Title Page
\maketitle
\tableofcontents
\newpage

%Introduction
\section{Introduction}\label{sec:intro}
Aim\\
This paper descibes the methodology and the results achieved by building an implementaiton on ros2 that controls a robotic arm in order to grab and move some randomly spawned blocks of different types. The aim of this project is to provide a functional and correct implementation to be able to perform the operation without encountering singularities or errors.\\
Strumentation\\
The simulated strumentation that was provided to us by the professor is multiple. The first is the camera settings, that is the position on which we base all the other ones. The second one is the UR5 robotic arm, which is a lightweight arm designed for tasks that require flexibility, it has 6 degrees of freedom: a base, a shoulder, and elbow and then a spherical wrist. A docker image with GUI capabilities was given to us, to handle the part of the setting of the arm and of the communication. The last one is the gripper and its script to open and close it to be able to grab the objects.\\
The tools used are DockerDesktop, ros2, gazebo, yolov5 and Roboflow...\\
Development environment\\
The algorithm developed is divided into two main parts: vision and manipulaition. The first is responsible of scanning the camera to grab an image, process it to localize all the blocks using Yolov5 and returning the coordinates of the bounding boxes to calculate the relative positions. The second controls the movement of the arm, so moves to the given position, grabs the object, and puts it in a designed position.


% Vision Section
\section{Vision}\label{sec:vision}
Vision plays a critical role in robotics by enabling the robot to perceive and interpret its environment. 
We started by analyzing the folders with the images provided by the professor. This brought us to three main challenges: spawn the blocks, recognize them, and calculate their position in the space. 
The objective of this section is to describe the objectives of the vision system, such as object detection, localization, or environment mapping.

\subsection{Block spawn}\label{subsec:blockspawn}

\begin{itemize}
    \item General position: The first thing to do was to understand whether we have to determine the position relatively to the table or to the camera. To understand this, we analyzed the camera settings which will be explained in the next point and we came up with the conclusion that the position was relative to the one of the camera.
    Another important thing which will be very useful also in the following parts is creating a table with each block and it's relative dimension. This was done by analyzing the files with the block prototypes.
    \item When launching the python the nodes are created when launching and so the camera, the table and the arm are created.\\
    To generate the blocks the .xacro file are generated and so there is the need to bring them to .sdf files, so gazebo can handle them.\\
    Random block algorithm: the blocks are generated in a random way between an array... Non so come siano generati e descrizione parametri...
    The number of generated blocks can be choose by the user by inserting the number in the command line when asked. The minumum number is 1 and we can generate up to 10 blocks. Blocks parameters like type and color are chosen randomly between an array and then the block is generated with its parameters plus the position and the orientation.
    The objects in the complete scenes are spawned in the following order [...]? camera, table, arm with gripper and blocks.
    \item Random position algorithm: the items are positioned in a random way on the table, this is done using a function to generate the positions between the limits and one to check for collisions. The blocks also have a rotation determined by another function.
\end{itemize}

\subsection{Processing images}\label{subsec:imageproc}
\begin{itemize}
    \item \textbf{Camera settings:} ... descrivere la posizione?\\
    The camera is setted in the position x=-0.5 , y=0.5, z=1.2, R=0.0, P=0.4, Y=0.06. The last three parameters are the rotation values.\\
    The image is captured from the camera and is saved in a predefined path so we can reach it. The image is also converted to 2D before being saved, so we can perform object recognition on it.
    \item \textbf{Object detection and classes:} For this part we used the tool Roboflow which allowed us to process the images and detect objects. The images used were the ones provided by the professor, excluding the .json files that were unuseful since we decided to create our own dataset from scratch. To do that, we uploaded the files on the tool and analyzed them folder by folder, telling the program which class it is. We ended up having eleven classes, each with a large number of images loaded. Then a percentage of imeges was destined to training, a smaller and a smaller one to testing, this was done automatically by the program. At the end of the processes the program output was our blocktrain.pt, which is the file containing the weights to be used to perform the recognition with Yolov5.
    \item \textbf{Machine learning model, YOLOv5:} we used Yolov5 to train our model. This was done using a Python script. The major part of this file was taken from the Yolo tutorial website and arranged to our needs[...]. 
    \item \textbf{TITLE:} When passing an image we were interested in knowing the coordinates of the bounding box, the confidence, and the class. With the first one we are able to calculate the position of the block on the table, with the second one we can understand wheter it is an erroneous identification and with the last one we can calculate how much we need to open the gripper, since some block are bigger than others. To obtain this information, we developed a Python script that performs the detection on a given image and returns back an array with the previously mentioned elements. To simplifies the call of the script, since we needed it every time we got an image, we decided to develop a service, so if a block is behind another, after removing the closer one we can see the one behind. To do that we had to integrate the code with some more instuctions to create a ros2 node and instantiate a service that receives and returns the correct information....\\
    The distance of the blocks is determined through a function that takes in input the coordinates of the bounding box as a 2D point and returns the centre. All the centres of the boxes are stored in an array and the first one, which will be the one processed, is passed to the Conversion service, which given the coordinates of the centre, determines the distance of the object with respect to the camera.
    The file \textbf{yolo.sh} file is the only one that implements the node a in python and this creates some problems...
\end{itemize}

\subsection{Detection}\label{subsec:detect}
\begin{itemize}
    \item When passing an image we were interested in knowing the coordinates of the bounding box, the confidence, and the class. With the first one we are able to calculate the position of the block on the table, with the second one we can understand wheter it is an erroneous identification and with the last one we can calculate how much we need to open the gripper, since some block are bigger than others. To obtain this information, we developed a Python script that creates a service: yolo\_bounding\_box\_service, which takes in input the path of the image from the camera and returns an array of custom msg type called Boundstruct, that contains the class identifier, the confidenxe and the four values of the coordinates of the top left and bottom right points of the bounding box.
\end{itemize}

\subsection{Object elaboration}\label{subsec:objel}
\begin{itemize}
    \item The values returned in \ref{subsec:detect}, in particular the related to the bounding box, are essential to derive the center of the object and compute the distance from the camera. This is done using a function that determines the center by subtracting the further point coordinate with the lower ones, divide the difference by two and add it to the lower one values. Then the coordinates of this point are passed to the Conversion service which returns the depth  
\end{itemize}

\subsection{Orientation}\label{subsec:orientation}
Reserved to Matteo...

% Manipulation Section
\section{Manipulation}\label{sec:manipulation}
The manipulation system allows the robot to interact with the surrounding environment and interact with objects through its end effector, which is a gripper in this case.
The objective of this part is to develop an algorithm that, given a position, can reach the object, grab it and take it to its assigned storage location.
\subsection{Direct and Inverse kinematics}\label{subsec:kinematics}
Two fundamental instruments to perform the task of moving the arm, are the direct and inverse kinematics. The first one allows us to compute the position and the orientation of the end effector given a set of Denavit-Hartenberg(DH) parameters, which are: theta, alpha, d and a. For each joint of the arm the matrix with the position and the rotation is computed witht the homogeneous transform and they are multiplied following the direct kinematics theory to obtain the T06 matrix. The function returns the position values as a vector and the rotation matrix.\\
On the other hand, inverse kinematics is the opposite process, by starting from a desired position and orientation of the end effector the aim is to compute the values of the DH parameters to reach it.

\subsection{Trajectory}\label{subsec:trajectory}
To compute the trajectory the first thing to do is to check wheter the destination is reachable. Then the function p2pMotionPlan takes the initial configuration of the robot, the position to reach with its orientation, the time of the movement and a matrix where to store the values. This computation is done by using the inverse kinematics explained in \ref{subsec:kinematics}, if no configuration is found, it means that there are no valid solutions for the requested configuration.

\subsection{Operational space using polynomials}\label{subsec:opspace}


\subsection{Design and Implementation}\label{subsec:design}
Discuss the manipulator's design, actuators, and control strategies. Include subsections as needed:
\begin{itemize}
    \item \textbf{End-effector design:} the end effector is a gripper as said in \ref{sec:manipulation}. In particular we have a two finger gripper, which is able to open and close, so it enables robots to perform specific tasks such as picking up, holding, moving, or placing objects. 
    \item \textbf{Kinematics and dynamics:} The first thing we have done was to analyze all the Matlab file to choose the implementation to use for the project. The options were two: the differential kinematics and the joint space. We decided to use the second one, because differential kinematics is really difficult to manage within this system in our opinion and so we avoided it to reduce the issues. To perform all the operations the Eigen library is used, it allows to convert the code from Matlab implementation to C++ in an easier and clearer way. We also added some checks to avoid singularities. 
    \item \textbf{Control algorithms (e.g., PID, MPC):} ...
\end{itemize}



% Workflow Section
\section{Workflow}\label{sec:workflow}
Describe the overall workflow of the project, integrating Vision and Manipulation. Most of the previously described functions are implemented as a service, so they are called whenever needed. Generally speaking the algorithm works by scanning with the camera and passing the image to the boundinboxservice, which returns a list with the content and the coordinate of the detected boxes. After that the localization service is called, to determiine the position that the arm has to reach. [...] Then the closest object is selected and the arm reaches it and opens the gripper and then closes it to grab it and bring it to the desired position where it is then released.


\subsection{System Architecture}\label{subsec:systemarchitecture}
Illustrate the system architecture with a diagram or flowchart. For example:
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{workflow_diagram.png} % Replace with your image filename
    \caption{System architecture diagram.}
    \label{fig:workflow_diagram}
\end{figure}


\subsection{Testing and Validation}\label{subsec:testing}
Summarize the testing procedures and validation results. Include tables, graphs, or images as necessary.

% Conclusion
\section{Conclusion}\label{sec:conclusion}
Summarize the achievements of the project, key findings, and potential future work. 


\end{document}
