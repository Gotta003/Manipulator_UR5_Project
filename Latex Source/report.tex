\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[hidelinks]{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{ulem}

% Title and Author
\title{Robotics Project}
\author{Benassi Alessandro, Calvo Daniele, Cristoforetti Niccolò, Gottardelli Matteo}
\date{\today}

% Begin Document
\begin{document}

% Title Page
\maketitle
\tableofcontents
\newpage

%Introduction
\section{Introduction}\label{sec:intro}
\textit{La divisione con i titoli in grassetto è solo per rendere chiare le parti, tutte le frasi sottolineate sono quelle che tendenzialmente cambierei al 100\%}\\
\textbf{Aim:} This paper describes the methodology and the results achieved by a ros2 based program that moves an UR5 robotic arm with a gripper, in order to grab and move some different randomly spawned blocks from a random start position to a desired position. The aim of this project is to provide a functional and correct implementation to be able to perform the operation without encountering singularities or errors.\\
\textbf{Strumentation:} There is a set of simulated strumentation provided by the professor to give us a base for the project. The first thing is the camera and its settings, which are the position values(and the services to get the image??) that we used as the absolute position(?). The second one is the Gazebo environment with the UR5 robotic arm, a lightweight arm designed for tasks that require flexibility, it has 6 degrees of freedom: a base, a shoulder, and elbow and then a spherical wrist. A docker image with GUI capabilities was given to us, to handle the part of the setting of the arm and of the communication. The last one is the gripper and its script to open and close it to be able to grab the objects.\\
...The tools used are DockerDesktop, ros2, gazebo, yolov5 and Roboflow...\\
\textbf{Development environment:} The algorithm developed is divided into two main parts: vision and manipulation. The first is responsible of scanning the camera to extract an image, process it to localize all the blocks using Yolov5 and returning the coordinates of the bounding boxes to calculate the relative positions. The second controls the movement of the arm to reach the given position, grab the object, and put it in the designed position.


% Vision Section
\section{Vision}\label{sec:vision}
Vision plays a critical role in robotics by allowing the robot to perceive and interpret its environment. 
The three main challenges are: spawning the blocks, recognizing them, and calculating the position in the space. 
[The objective of this section is to describe the objectives of the vision system, such as object detection, localization, or environment mapping.]

\subsection{Block spawn}\label{subsec:blockspawn}
As mentioned in \ref{sec:intro} the absolute position is the one of the camera, given by the following parameters: x=-0.5 , y=0.5, z=1.2, R=0.0, P=0.4, Y=-0.06, the first three are the coordinates, the others are the rotation values. At this point we had the table with the arm attached and we wanted to spawn the blocks in the right part, which is closer to the camera. The blocks are spawned directly in the sim.launch.py so they are spawned right after the creation of the environment with the table and the arm(non del tutto vero guardando LaunchDescription). The minimum number of blocks is 1 and it goes up to 5 and this parameter is inserted by the user in the command line. The spawn area of the blocks has a fixed height, which is the same of the table(z=0.88) and its width in the x and y axis is respectively (0.05, 0.405) and (0.2, 0.58). All the generated blocks are different in type and color. The type is decided randomly between a set of nine, taken from the ones in the folder published on moodle[...], which were eleven, but we decided to remove two of them that are larger than the single block to avoid issues with the gripper, that may not open enough to get around the block[grab it correctly per riassumere in caso][since the script we are in posses only allows us to open and close and not to decide a width...]. The color is also chosen in a random way between a selected set and, regarding the position, it is generated randomly and has to stay within the limits of the defined spawn area and be collision free, so there is a minimum distance between each object, otherwise we regenerate the overlapping one. The orientation is random and is only about the roll(z axis), because the project does not consider the blocks in a different position, like upside down, for a matter of complications in detection and grabbing.
The objects are generated using a .urdf.xacro file, which is then passed to .urdf format and at the end in .sdf to be displayable on Gazebo. 


\subsection{Processing images}\label{subsec:imageproc}
To perform object recognition the first thing to do is to scan an image from the camera. This is done by the node image\_acquiring, which subscribes to the topic that publishes \textit{sensor\_msgs::msg::Image} data and after gathering the content it is processed with opencv2 library functions to print it on a generated image file in .png format. It is saved in a precise path that will be used later in the program.\\Since the recognition is performed at every complete movement of the arm, the part of the table with the blocks that have already been moved is darkened to avoid the recognition and the movement of these. Furthermore, the background is colored black, to allow the object recognition algorithm to perform at its best...

\subsection{Detection}\label{subsec:detect}
For this part we used the tool Roboflow which allowed us to process the images and detect objects in order to produce a pre-trained weight file for Yolov5. \uline{The images used were the ones provided by the professor, excluding the .json files that were unuseful since we decided to create our own dataset from scratch.} To do that, we uploaded the files on the tool and analyzed them folder by folder, telling the program which class it is. We ended up having nine classes(see \ref{subsec:blockspawn}), each with a \uline{large} number of images loaded. Then a percentage of images was destined to training, and a smaller one to testing, this was done automatically by the program, which performed also the training part returning. At the end of the processes the program output was our \uline{blocktrain.pt}, which is the file containing the pre-trained weights to be used to perform the recognition with Yolov5. For the recognition we \uline{downloaded} the yolov5-pip repository from github with this link \url{https://github.com/fcakyon/yolov5-pip.git}.\\
When passing an image we were interested in knowing the coordinates of the bounding box, the confidence, and the class. With the first one we are able to calculate the position of the block on the table and with the second one we can understand whether it is an erroneous identification \uline{class use missing, maybe for position where to move but still to be implemented}. To obtain this information, we developed a Python script detection.py that creates the service: yolo\_bounding\_box\_service, which takes in input the path of the image from the camera as explained in \ref{subsec:imageproc} and returns an array of custom msg type called Boundstruct, that contains the class identifier, the confidence and the four values of the coordinates of the top left and bottom right points of the bounding box.

% Object elaboration Section
\subsection{Object elaboration}\label{subsec:objel}
The values returned in \ref{subsec:detect}, in particular the related to the bounding box, are essential to derive the center of the object and compute the distance from the camera. This is done using a function that determines the center by subtracting the further point coordinate with the lower ones, divide the difference by two and add it to the lower one values. Then the coordinates of this point are passed to the Conversion service which returns the depth. With this process we obtain the coordinates of the block with respect to the camera \uline{then we have to transform it to the real world frame...(logica sebe da vedere)}. \\

Add orientation...  


% Manipulation Section
\section{Manipulation}\label{sec:manipulation}
The manipulation system allows the robot to interact with the surrounding environment and interact with objects through its end effector, which is a gripper in this case.
The objective of this part is to develop an algorithm that, given a position, can reach the object, grab it and take it to its assigned storage location.
\subsection{Direct and Inverse kinematics}\label{subsec:kinematics}
Two fundamental instruments to perform the task of moving the arm, are the direct and inverse kinematics. The first one allows us to compute the position and the orientation of the end effector given a set of Denavit-Hartenberg(DH) parameters, which are: theta, alpha, d and a. For each joint of the arm the matrix with the position and the rotation is computed with the homogeneous transform and then it is multiplied following the direct kinematics theory to obtain the T06 matrix. The function returns the position values as a vector and the rotation matrix.\\
On the other hand, inverse kinematics is the opposite process, by starting from a desired position and orientation of the end effector the aim is to compute the values of the DH parameters to reach it.

\subsection{Trajectory}\label{subsec:trajectory}
To compute the trajectory the first thing to do is to check wheter the destination is reachable. After that there are 3 different ways to proceed:
\begin{itemize}
    \item Operation space with the interpolating points: the trajectory is calculated specifying a series of waypoints, which are then lined up with an algorithm to obtain a linear path.
    \item differential kinematics: it is based on the relation between the joints velocity and the velocity of the end effector. The Jacobian matrix is used to map from the joint space to the operational space and then the pseudoinverse is used to compute the joint configurations over time. This is computationally expensive an can lead to problems when singularities are encountered, moreover we wanted to decide the velocity of the movement arbitrarily so this method doesn't fit with our needs because this last is controlled by the robot's security systems, which limit it.
    \item Operation space with positions: it is similar to the first one, but instead of interpolating the points, the desired positions are directly specified in the operational space and it is verified that the end effector reaches them. The direct and inverse kinematics are used to compute the point to reach and the configurations to adopt in order to do it. 
\end{itemize}    
We choose this last implementation and to guarantee a smoother trajectory a cubic spline is added.(aggiungere motivazioni)

\subsection{Operational space using polynomials}\label{subsec:opspace}
The function p2pMotionPlan() computes the path and to do that it takes: the initial configuration of the robot, the position to reach with its orientation, the time of the movement and a matrix where to store the values. This computation is done by using the inverse kinematics explained in \ref{subsec:kinematics}; if no configuration is found, it means that there are no valid solutions for the requested configuration. Third grade polynomials are used to obtain a smooth trajectory and avoid sudden accelerations or oscillations. The matrix containing the move parameters(destination and rotation) is directly modified by reference and the function return a boolean to tell if the operation is successful or not.

\subsection{Constraints(reachable position, angles and space limits)}\label{subsec:constraints}
\textbf{DA SISTEMARE} Values that are close to 0 are adjusted to 0 with adjust\_value().\\
for the asin function values are adjuste so that if $<-1, -\pi/2$ is returned, while if $>1, \pi/2$ is returned, otherwise the values remain unchanged.\\
for the acos function if $<-1, \pi$ is returned, if $>1, 0$ is returned.\\
Space limits: XMIN=-1 , XMAX=1; YMIN=-1 , YMAX(i)=i$<$NUM\_JOINTS/2 ? 0.3 : 0.2 ; ZMIN=0.1 , ZMAX=0.9\\
Bisongna dire che le posizioni che vogliamo raggiungere sono quelle sul tavolo o abbiamo limitazioni?

\subsection{Logic motion}\label{subsec:logic}
The process of the single block motion is the following: the arm starts in the home position, it opens the gripper and goes down over the block in a security position(-0.5?check), it then lowers down and closes the fingers to grab the block, after that it stops in the security position. Then it moves to the designed position, it goes down to the security position and then lowers on the table surface and releases the block. Finally it goes up to the security position over the object and then it returns to the home configuration before restarting with the visualization part and iterating until all the objects are moved.

% Conclusion
\section{Conclusion}\label{sec:conclusion}
Summarize the achievements of the project, key findings, and potential future work. 


\end{document}
