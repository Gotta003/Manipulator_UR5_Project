\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[hidelinks]{hyperref}
\usepackage{geometry}
\geometry{margin=1in}

% Title and Author
\title{Robotics Project}
\author{Benassi Alessandro, Calvo Daniele, Cristoforetti Niccol√≤, Gottardelli Matteo}
\date{\today}

% Begin Document
\begin{document}

% Title Page
\maketitle
\tableofcontents
\newpage

%Introduction
\section{Introduction}\label{sec:intro}
This paper descibes the methodology and the results achieved by building an implementaiton on ros2 that controls a robotic arm in order to grab and move some randomly spawned blocks. The tools used are DockerDesktop, ros2, gazebo, yolov5 and roboflow. The algorithm developed is divided into two main parts: vision and manipulaition. The first is responsible of scanning the camera to grab an image, process it to localize all the blocks using yolov5 and returning the coordinates of the bounding boxes to calculate the relative positions. The second controls the movement of the arm, so moves to the given position, grabs the object and puts it in a designed position.
The aim of this project is to provide a functional and correct implementation to be able to perform the operation without errors or singularities.

% Vision Section
\section{Vision}\label{sec:vision}
Vision plays a critical role in robotics by enabling the robot to perceive and interpret its environment. 
We started by analyzing the folders with the images provided by the professor. This brought us to three main challenges: spawn the blocks, recognize them, and calculate their position in the space. 
The objective of this section is to describe the objectives of the vision system, such as object detection, localization, or environment mapping.

\subsection{Block spawn}\label{subsec:blockspawn}

\begin{itemize}
    \item General position: The first thing to do was to understand whether we have to determine the position relatively to the table or to the camera. To understand this, we analyzed the camera settings which will be explained in the next point and we came up with the conclusion that the position was relative to the one of the camera.
    Another important thing which will be very useful also in the following parts is creating a table with each block and it's relative dimension. This was done by analyzing the files with the block prototypes.
    \item When launching the python the nodes are created when launching and so the camera, the table and the arm are created.\\
    To generate the blocks the .xacro file are generated and so there is the need to bring them to .sdf files, so gazebo can handle them.\\
    Random block algorithm: the blocks are generated in a random way between an array... Non so come siano generati e descrizione parametri...
    The number of generated blocks can be choose by the user by inserting the number in the command line when asked. The minumum number is 1 and we can generate up to 10 blocks. Blocks parameters like type and color are chosen randomly between an array and then the block is generated with its parameters plus the position and the orientation.
    The objects in the complete scenes are spawned in the following order [...]? camera, table, arm with gripper and blocks.
    \item Random position algorithm: the items are positioned in a random way on the table, this is done using a function to generate the positions between the limits and one to check for collisions. The blocks also have a rotation determined by another function.
\end{itemize}

\subsection{Processing images}\label{subsec:imageproc}
\begin{itemize}
    \item \textbf{Camera settings:} ... descrivere la posizione?\\
    The camera is setted in the position x=-0.5 , y=0.5, z=1.2, R=0.0, P=0.4, Y=0.06. The last three parameters are the rotation values.
    \item \textbf{Object detection and classes:} For this part we used the tool Roboflow which allowed us to process the images and detect objects. The images used were the ones provided by the professor, excluding the .json files that were unuseful since we decided to create our own dataset from scratch. To do that, we uploaded the files on the tool and analyzed them folder by folder, telling the program which class it is. We ended up having eleven classes, each with a large number of images loaded. Then a percentage of imeges was destined to training, a smaller and a smaller one to testing, this was done automatically by the program. At the end of the processes the program output was our blocktrain.pt, which is the file containing the weights to be used to perform the recognition with Yolov5.
    \item \textbf{Machine learning model, YOLOv5:} we used Yolov5 to train our model. This was done using a Python script. The major part of this file was taken from the Yolo tutorial website and arranged to our needs[...]. 
    \item \textbf{TITLE:} When passing an image we were interested in knowing the coordinates of the bounding box, the confidence, and the class. With the first one we are able to calculate the position of the block on the table, with the second one we can understand wheter it is an erroneous identification and with the last one we can calculate how much we need to open the gripper, since some block are bigger than others. To obtain this information, we had to develop a Python script that performs the detection on a given image and returns back an array with the previously mentioned elements. To simplifies the call of the script, since we needed it every time we got an image, we decided to develop a service, so if a block is behind another, after removing the closer one we can see the one behind. To do that we had to integrate the code with some more instuctions to create a ros2 node and instantiate a service that receives and returns the correct information....\\
    The file \textbf{yolo.sh} file is the only one that implements the node a in python and this creates some problems...
\end{itemize}

% Manipulation Section
\section{Manipulation}\label{sec:manipulation}
The manipulation system allows the robot to interact with the sorrounding environment and interact with objects through its end effector, which is a gripper in this case.
The objective of this part is to develop an algorithm that, given a position, can reach the object, grab it and take it to its assigned storage location.

\subsection{Design and Implementation}\label{susec:design}
Discuss the manipulator's design, actuators, and control strategies. Include subsections as needed:
\begin{itemize}
    \item \textbf{End-effector design:} the end effector is a gripper as said in \ref{sec:manipulation}. In particular we have a two finger gripper, which is able to open and close, so it enables robots to perform specific tasks such as picking up, holding, moving, or placing objects. 
    \item \textbf{Kinematics and dynamics:} we decided to use the joint space because differential kinematics is really difficult to manage with this system in our opinion and so we avoided it to reduce the issues.
    \item \textbf{Control algorithms (e.g., PID, MPC):} ...
\end{itemize}



% Workflow Section
\section{Workflow}\label{sec:workflow}
Describe the overall workflow of the project, integrating Vision and Manipulation. Most of the previously described functions are implemented as a service, so they are called whenever needed. Generally speaking the algorithm works by scanning with the camera and passing the image to the boundinboxservice, which returns a list with the content and the coordinate of the detected boxes. After that the localization service is called, to determiine the position that the arm has to reach. [...] Then the closest object is selected and the arm reaches it and opens the gripper and then closes it to grab it and bring it to the desired position where it is then released.


\subsection{System Architecture}\label{subsec:systemarchitecture}
Illustrate the system architecture with a diagram or flowchart. For example:
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{workflow_diagram.png} % Replace with your image filename
    \caption{System architecture diagram.}
    \label{fig:workflow_diagram}
\end{figure}

\subsection{Integration}\label{subsec:integration}
Discuss how different modules (e.g., Vision and Manipulation) are integrated. Highlight communication protocols, data flow, and synchronization.

\subsection{Testing and Validation}\label{subsec:testing}
Summarize the testing procedures and validation results. Include tables, graphs, or images as necessary.

% Conclusion
\section{Conclusion}\label{sec:conclusion}
Summarize the achievements of the project, key findings, and potential future work. 


\end{document}
